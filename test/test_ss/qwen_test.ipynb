{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "device = \"cuda\"  # the device to load the model onto\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"/home/ss/models/Qwen2-7B-Instruct\", torch_dtype=\"auto\", device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/ss/models/Qwen2-7B-Instruct\")\n",
    "\n",
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "generated_ids = model.generate(model_inputs.input_ids, max_new_tokens=512)\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids) :]\n",
    "    for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from typing import (\n",
    "    AbstractSet,\n",
    "    cast,\n",
    "    Collection,\n",
    "    Dict,\n",
    "    Iterator,\n",
    "    List,\n",
    "    Literal,\n",
    "    Sequence,\n",
    "    TypedDict,\n",
    "    Union,\n",
    ")\n",
    "device = \"cuda\" # the device to load the model onto\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     \"/home/ss/models/Qwen2-7B-Instruct\",\n",
    "#     torch_dtype=\"auto\",\n",
    "#     device_map=\"cuda:0\"\n",
    "# )\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/ss/models/Qwen2-7B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.vocab_size #151643\n",
    "len(tokenizer) #151646 = 151643 + 3(special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "a = [151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 68990, 73670, 104682, 101314, 151645, 198, 151644, 77091, 198, 68990, 100622, 105538, 106114, 3837, 20412, 116498, 5373, 99348, 104653, 99490, 3837, 103926, 104087, 100760, 104682, 9370, 105869, 1773, 87752, 99639, 97084, 68990, 105891, 99790, 105869, 33108, 99348, 112765, 48443, 16, 13, 3070, 113508, 9909, 101168, 100085, 59074, 7552, 334, 5122, 105196, 115339, 77540, 30540, 9370, 109325, 117485, 3837, 100000, 102506, 118261, 113501, 5373, 46485, 105200, 105896, 115534, 100166, 99470, 99893, 100653, 3407, 17, 13, 3070, 35727, 50285, 64689, 104208, 334, 5122, 103987, 68990, 9370, 99488, 3837, 20412, 102506, 104003, 99490, 104208, 100653, 3837, 102385, 18830, 35727, 50285, 64689, 59074, 99432, 5373, 99532, 104182, 118929, 49567, 3407, 18, 13, 3070, 105752, 334, 5122, 68990, 37474, 105752, 100630, 99568, 93488, 102288, 105752, 5373, 101576, 99810, 118996, 105752, 5373, 108715, 53938, 105752, 49567, 3837, 20412, 99489, 108077, 3837, 106411, 99164, 105538, 118015, 33108, 16530, 102683, 3407, 100001, 100371, 99902, 108869, 68990, 9370, 106361, 33108, 100022, 3837, 100000, 99794, 58695, 33108, 99489, 113620, 101945, 105271, 1773, 151645, 198, 151644, 872, 198, 29437, 2, 18, 18947, 100371, 52801, 109333, 151645, 198, 151644, 77091, 198]\n",
    "b = [151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 68990, 73670, 104682, 101314, 151645, 198, 151644, 77091, 198, 68990, 100622, 105538, 106114, 3837, 20412, 116498, 5373, 99348, 104653, 99490, 3837, 103926, 104087, 100760, 104682, 9370, 105869, 1773, 87752, 99639, 97084, 68990, 105891, 99790, 105869, 33108, 99348, 112765, 48443, 16, 13, 3070, 113508, 9909, 101168, 100085, 59074, 7552, 334, 5122, 105196, 115339, 77540, 30540, 9370, 109325, 117485, 3837, 100000, 102506, 118261, 113501, 5373, 46485, 105200, 105896, 115534, 100166, 99470, 99893, 100653, 3407, 17, 13, 3070, 35727, 50285, 64689, 104208, 334, 5122, 103987, 68990, 9370, 99488, 3837, 20412, 102506, 104003, 99490, 104208, 100653, 3837, 102385, 18830, 35727, 50285, 64689, 59074, 99432, 5373, 99532, 104182, 118929, 49567, 3407, 18, 13, 3070, 105752, 334, 5122, 68990, 37474, 105752, 100630, 99568, 93488, 102288, 105752, 5373, 101576, 99810, 118996, 105752, 5373, 108715, 53938, 105752, 49567, 3837, 20412, 99489, 108077, 3837, 106411, 99164, 105538, 118015, 33108, 16530, 102683, 3407, 100001, 100371, 99902, 108869, 68990, 9370, 106361, 33108, 100022, 3837, 100000, 99794, 58695, 33108, 99489, 113620, 101945, 105271, 1773, 151645, 198, 151644, 872, 198, 29437, 2, 18, 18947, 100371, 52801, 109333, 151645, 198, 151644, 77091, 198]\n",
    "print(tokenizer.decode(a))\n",
    "\n",
    "1 *故宫*\n",
    "\n",
    "1 宫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Role = Literal[\"system\", \"user\", \"assistant\"]\n",
    "\n",
    "class Message(TypedDict):\n",
    "    role: Role\n",
    "    content: str\n",
    "\n",
    "Dialog = Sequence[Message]\n",
    "\n",
    "class TokenizerHF:\n",
    "    def __init__(self, path: str):\n",
    "        self.model = AutoTokenizer.from_pretrained(path)\n",
    "        # self.model = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-7B-Instruct\")\n",
    "        # Qwen2 don't set bos but have <|im_start|>\n",
    "        # all special tokens: <|endoftext|> <|im_start|> <|im_end|>\n",
    "        if \"qwen2\" in path.lower():\n",
    "            self.bos_id = self.model.convert_tokens_to_ids(\"<|im_start|>\")\n",
    "        else:\n",
    "            self.bos_id = self.model.bos_token_id\n",
    "        self.eos_id = self.model.eos_token_id\n",
    "        self.pad_id = self.model.pad_token_id\n",
    "        self.stop_tokens = self.model.eos_token_id\n",
    "        self.n_words = self.model.vocab_size\n",
    "\n",
    "    def encode(self, s: str, bos: bool, eos: bool) -> List[int]:\n",
    "        t = self.model.encode(s, add_special_tokens=False)\n",
    "        if bos:\n",
    "            t.insert(0, self.bos_id)\n",
    "        if eos:\n",
    "            t.append(self.eos_id)\n",
    "        return t\n",
    "\n",
    "    def decode(self, t: Sequence[int], skip_special_tokens=False) -> str:\n",
    "        return self.model.decode(t, skip_special_tokens=skip_special_tokens)\n",
    "\n",
    "\n",
    "class ChatFormatHF:\n",
    "    def __init__(self, tokenizer: TokenizerHF):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def encode_header(self, message: Message) -> List[int]:  # ???\n",
    "        tokens = []\n",
    "        # tokens.append(self.tokenizer.bos_id)\n",
    "        tokens.extend(self.tokenizer.encode(message[\"role\"], bos=True, eos=False))\n",
    "        # tokens.append(self.tokenizer.eos_id)\n",
    "        tokens.extend(self.tokenizer.encode(\"\\n\", bos=False, eos=False))\n",
    "        return tokens\n",
    "\n",
    "    def encode_message(self, message: Message) -> List[int]:\n",
    "        tokens = self.encode_header(message)\n",
    "        tokens.extend(\n",
    "            self.tokenizer.encode(message[\"content\"].strip(), bos=False, eos=True)\n",
    "        )\n",
    "        # tokens.append(self.tokenizer.eos_id)\n",
    "        tokens.extend(self.tokenizer.encode(\"\\n\", bos=False, eos=False))\n",
    "        return tokens\n",
    "\n",
    "    def encode_dialog_prompt(self, dialog: Dialog) -> List[int]:\n",
    "        tokens = []\n",
    "        # tokens.append(self.tokenizer.bos_id)\n",
    "        for message in dialog:\n",
    "            tokens.extend(self.encode_message(message))\n",
    "        # Add the start of an assistant message for the model to complete.\n",
    "        tokens.extend(self.encode_header({\"role\": \"assistant\", \"content\": \"\"}))  # ???\n",
    "        return tokens\n",
    "\n",
    "tok = TokenizerHF(\"/home/ss/models/Qwen2-7B-Instruct\")\n",
    "cf = ChatFormatHF(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "print(tok.decode(cf.encode_dialog_prompt(messages)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cinfer_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
