stages:
    - format
    - env
    - check

variables:
    VENV: "/home/gitlab-runner/venv-cinfer-ci"
    GIT_SUBMODULE_STRATEGY: recursive

before_script:
    - |
      if [ ! -d "$VENV" ]; then
        python3 -m venv "$VENV"
      fi
    - source $VENV/bin/activate

format_check:
    stage: format
    only:
        - merge_requests
    tags:
        - dev
    script:
        - pip install black -i https://pypi.tuna.tsinghua.edu.cn/simple
        - black --exclude src/third_party/ --exclude third_party/ --check .

env_build_nvidia:
    stage: env
    only:
        - merge_requests
    tags:
        - dev
    needs:
        - format_check
    script:
        - export CUDA_HOME=/usr/local/cuda-12.4
        - export LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH
        - export PATH=$CUDA_HOME/bin:$PATH
        - pip install -r requirements-build.txt -i https://pypi.tuna.tsinghua.edu.cn/simple
        - TORCH_CUDA_ARCH_LIST=8.6 CINFER_SETUP_JOBS=4 MAX_JOBS=4 CINFER_WITH_CYTHON=1 pip install --no-build-isolation -i https://pypi.tuna.tsinghua.edu.cn/simple .[quant]


env_build_muxi:
    stage: env
    only:
        - merge_requests
    tags:
        - muxi
    needs:
        - format_check
    script:
        - export PATH=/opt/maca/mxgpu_llvm/bin:/opt/maca/bin:$PATH
        - export MACA_PATH=/opt/maca
        - export LD_LIBRARY_PATH=/opt/maca/lib:/opt/maca/mxgpu_llvm/lib:$LD_LIBRARY_PATH
        - pip install -U /home/share/software/maca-pytorch2.1-py38-2.25.2.8-x86_64/2.25.2.8/wheel/*.whl -i https://pypi.tuna.tsinghua.edu.cn/simple
        - pip install -r requirements-build.txt -i https://pypi.tuna.tsinghua.edu.cn/simple
        - CINFER_SETUP_JOBS=4 MAX_JOBS=4 CINFER_WITH_CYTHON=1 pip install --no-build-isolation -i https://pypi.tuna.tsinghua.edu.cn/simple .


paged_check_nvidia:
    stage: check
    only:
        - merge_requests
    tags:
        - dev
    needs:
        - env_build_nvidia
    script:
      - grun torchrun --nproc_per_node 1 --master_port=22514 test/single_req_test.py request.max_new_tokens=64 infer.cache_type=paged


paged_check_muxi:
    stage: check
    only:
        - merge_requests
    tags:
        - muxi
    needs:
        - env_build_muxi
    script:
      - export PATH=/opt/maca/mxgpu_llvm/bin:/opt/maca/bin:$PATH
      - export MACA_PATH=/opt/maca
      - export LD_LIBRARY_PATH=/opt/maca/lib:/opt/maca/mxgpu_llvm/lib:$LD_LIBRARY_PATH
      - grun torchrun --nproc_per_node 1 --master_port=22514 test/single_req_test.py request.max_new_tokens=64 infer.cache_type=paged


skew_check_nvidia:
    stage: check
    only:
        - merge_requests
    tags:
        - dev
    needs:
        - env_build_nvidia
    script:
      - grun torchrun --nproc_per_node 1 --master_port=22515 test/single_req_test.py request.max_new_tokens=64 infer.cache_type=skew


skew_check_muxi:
    stage: check
    only:
        - merge_requests
    tags:
        - muxi
    needs:
        - env_build_muxi
    script:
      - export PATH=/opt/maca/mxgpu_llvm/bin:/opt/maca/bin:$PATH
      - export MACA_PATH=/opt/maca
      - export LD_LIBRARY_PATH=/opt/maca/lib:/opt/maca/mxgpu_llvm/lib:$LD_LIBRARY_PATH
      - grun torchrun --nproc_per_node 1 --master_port=22515 test/single_req_test.py request.max_new_tokens=64 infer.cache_type=skew


tp_paged_check_nvidia:
    stage: check
    only:
        - merge_requests
    tags:
        - dev
    needs:
        - env_build_nvidia
    script:
      - grun 2 torchrun --nproc_per_node 2 --master_port=22516 test/single_req_test.py request.max_new_tokens=64 infer.cache_type=paged infer.parallel_type=tensor


pp_paged_check_nvidia:
    stage: check
    only:
        - merge_requests
    tags:
        - dev
    needs:
        - env_build_nvidia
    script:
      - grun 2 torchrun --nproc_per_node 2 --master_port=22517 test/single_req_test.py request.max_new_tokens=64 infer.cache_type=paged infer.parallel_type=pipe


glm4_9b_nvidia:
    stage: check
    only:
        - merge_requests
    tags:
        - dev
    needs:
        - env_build_nvidia
    script:
      - grun torchrun --nproc_per_node 1 --master_port=22518 test/single_req_test.py request.max_new_tokens=64 infer.cache_type=paged models=glm4-9b


llama3_8b_nvidia:
    stage: check
    only:
        - merge_requests
    tags:
        - dev
    needs:
        - env_build_nvidia
    script:
      - grun torchrun --nproc_per_node 1 --master_port=22519 test/single_req_test.py request.max_new_tokens=64 infer.cache_type=paged models=llama3-8b infer.max_seq_len=2048


mixtral_8x7b_nvidia:
    stage: check
    only:
        - merge_requests
    tags:
        - dev
    needs:
        - env_build_nvidia
    script:
        - grun 5 torchrun --nproc_per_node 5 --master_port=22520 test/single_req_test.py request.max_new_tokens=64 infer.cache_type=paged infer.parallel_type=pipe models=mixtral-8x7b


gptq_nvidia:
    stage: check
    only:
        - merge_requests
    tags:
        - dev
    needs:
        - env_build_nvidia
    script:
      - CONFIG_NAME=serve_config_gptq grun torchrun --nproc_per_node 1 --master_port=22521 test/single_req_test.py request.max_new_tokens=64
