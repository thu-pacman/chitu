name: mixtral-8x7b
type: hf-mixtral
ckpt_dir: /home/share/models/Mixtral-8x7B-Instruct-v0.1
tokenizer_path: /home/share/models/Mixtral-8x7B-Instruct-v0.1 # TODO
dim: 4096
n_layers: 32
n_heads: 32
n_kv_heads: 8
vocab_size: 32000
intermediate_dim: 14336
norm_eps: 1e-05
rope_theta: 1000000.0
num_local_experts: 8
num_experts_per_tok: 2
qkv_has_bias: false
tokenizer_force_full_seq_decode: true
max_position_embeddings: 32768
max_seq_len: 10240 # length of prefill + decode
